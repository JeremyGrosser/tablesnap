#!/usr/bin/env python
import pyinotify
import boto

from optparse import OptionParser
from traceback import format_exc
from threading import Thread
import logging
import os.path
import socket
import json
import sys
import os


log = logging.getLogger('tablesnap')
stderr = logging.StreamHandler()
stderr.setFormatter(logging.Formatter('%(asctime)s %(levelname)s %(message)s'))
log.addHandler(stderr)
log.setLevel(logging.DEBUG)


class UploadHandler(pyinotify.ProcessEvent):
    def my_init(self, bucket=None):
        self.bucket = bucket
        self.hostname = socket.getfqdn()

    def process_IN_MOVED_TO(self, event):
        if event.pathname.find('-tmp') == -1:
            keyname = '%s:%s' % (self.hostname, event.pathname)
            key = self.bucket.get_key(keyname)
            if key is None:
                t = Thread(target=self.upload_sstable, args=(
                           keyname, event.pathname))
                t.setDaemon(True)
                t.start()
            else:
                log.info("Keyname %s already exists, skipping upload" % keyname)

    def upload_sstable(self, keyname, filename, with_index=True):
        log.info('Uploading %s' % filename)

        def progress(sent, total):
            if sent == total:
                log.info('Finished uploading %s' % filename)

        try:
            dirname = os.path.dirname(filename)
            if with_index:
                key = self.bucket.new_key('%s-listdir.json' % keyname)
                key.set_contents_from_string(
                    json.dumps({dirname: os.listdir(dirname)}))

            key = self.bucket.new_key(keyname)
            key.set_contents_from_filename(filename, replace=False, cb=progress)
        except:
            log.error('Error uploading %s\n%s' % (keyname, format_exc()))


def backup_files(handler, bucket, paths):
    for path in paths:
        log.info('Backing up %s' % path)
        for filename in os.listdir(path):
            if filename.find('-tmp') != -1:
                continue
            keyname = '%s:%s/%s' % (handler.hostname, path, filename)
            if not bucket.get_key(keyname):
                handler.upload_sstable(keyname, '%s/%s' % (path, filename), with_index=False)
            else:
                log.info('Not uploading %s/%s, it already exists in this S3 bucket.' % (path, filename))
    return 0


def main():
    parser = OptionParser(usage='%prog [options] <bucket> <path> [...]')
    parser.add_option('-k', '--aws-key', dest='aws_key', default=None)
    parser.add_option('-s', '--aws-secret', dest='aws_secret', default=None)
    parser.add_option('-r', '--recursive', action='store_true', dest='recursive', default=False,
        help='Recursively watch the given path(s)s for new SSTables')
    parser.add_option('-a', '--auto-add', action='store_true', dest='auto_add', default=False,
        help='Automatically start watching new subdirectories within path(s)')
    parser.add_option('-B', '--backup', action='store_true', dest='backup',
        help='Backup existing SSTables to S3 if they\'re not already there')
    options, args = parser.parse_args()

    if len(args) < 2:
        parser.print_help()
        return -1

    bucket = args[0]
    paths = args[1:]

    s3 = boto.connect_s3(options.aws_key, options.aws_secret)
    bucket = s3.get_bucket(bucket)
    handler = UploadHandler(bucket=bucket)

    if options.backup:
        return backup_files(handler, bucket, paths)

    wm = pyinotify.WatchManager()
    notifier = pyinotify.Notifier(wm, handler)
    for path in paths:
        ret = wm.add_watch(path, pyinotify.ALL_EVENTS, rec=options.recursive, auto_add=options.auto_add)
        if ret[path] == -1:
            log.critical('add_watch failed for %s, bailing out!' % path)
            return 1
    notifier.loop()

if __name__ == '__main__':
    sys.exit(main())
